[
  {
    "objectID": "stat-541-posts/2025-04-30-lab-2/index.html",
    "href": "stat-541-posts/2025-04-30-lab-2/index.html",
    "title": "Lab 2 - Advanced Data Visualizations",
    "section": "",
    "text": "View the code on GitHub"
  },
  {
    "objectID": "stat-541-posts/2025-04-30-lab-2/index.html#part-one-identifying-bad-visualizations",
    "href": "stat-541-posts/2025-04-30-lab-2/index.html#part-one-identifying-bad-visualizations",
    "title": "Lab 2 - Advanced Data Visualizations",
    "section": "Part One: Identifying Bad Visualizations",
    "text": "Part One: Identifying Bad Visualizations\n\nI believe that this graph is showing that different countries have different proportions of individuals who believe that vaccines are safe. While most of the region medians fall around 85, the median percentage is considerably lower for Europe and the Former Soviet Union.\nWe have region, country, and percentage of country that believes vaccines are safe.\nx-axis: percentage of people who believe vaccines are safe, color: global region, “y-axis:” region (to produce the stacking).\nGeom_point; a dotplot\n\n\n\nThe points are trending upwards for seemingly no reason, which is misleading. I would do a boxplot or a density plot in order to better convey the message of the distributions within the regions.\nIf we’re trying to look at general trends between regions, it doesn’t make sense to label individual countries. I will remove these.\nThere is no reason to re-label the regions. I will remove the legend.\n\n\n\n\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(leaflet)\nlibrary(rnaturalearth)\nlibrary(plotly)\n\n\ncountry_data &lt;- read_excel(\n  here::here(\"data\", \"wgm2018-dataset-crosstabs-all-countries.xlsx\"), \n                           sheet = \"Full dataset\")\n\ncountry_data_dictionary &lt;- read_excel(\n  here::here(\"data\", \"wgm2018-dataset-crosstabs-all-countries.xlsx\"), \n                           sheet = \"Data dictionary\")\n\n\ndictionary_firstrow &lt;- head(country_data_dictionary, n = 1)\n\nvariable_codes_list &lt;- as.list(str_split(dictionary_firstrow$`Variable Type & Codes*`, pattern = \",\"))\n\nvariable_codes_tibble &lt;- tibble(Code = str_trim(variable_codes_list[[1]]))\n\ncoding &lt;- variable_codes_tibble |&gt; \n  filter(str_trim(Code) != \"\") |&gt; \n  separate_wider_delim(Code, delim = \"=\", names_sep = \"Country\") |&gt; \n  rename(WP5 = \"CodeCountry1\", Country = \"CodeCountry2\") |&gt; \n  mutate(WP5 = as.numeric(WP5))\n\n\nfinal_dataset &lt;- cleaned_dataset |&gt; \n  group_by(Country, Regions_Report) |&gt; \n  summarize(\n    prop_safe_belief = sum(Q25 %in% c(1, 2), na.rm = TRUE) / sum(!is.na(Q25)),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(true_region = case_when(\n    Regions_Report %in% c(3, 13) ~ \"Middle East and North Africa\",\n    Regions_Report %in% c(9, 10, 11, 12, 18) ~ \"Asia\",\n    Regions_Report %in% c(1, 2, 4, 5) ~ \"Sub-Saharan Africa\",\n    Regions_Report %in% c(6, 7, 8) ~ \"Americas\",\n    Regions_Report %in% c(15, 16, 17) ~ \"Europe\",\n    Regions_Report == 14 ~ \"Former Soviet Union\",\n    TRUE ~ \"Not Assigned\"\n  )) |&gt; \n  filter(true_region != \"Not Assigned\",\n         !is.na(prop_safe_belief)) |&gt; \n  mutate(prop_safe_belief_percent = prop_safe_belief * 100) \n\n\nggplot(data = final_dataset, \n       mapping = aes(x = fct_reorder(true_region, prop_safe_belief_percent, .fun = median), \n                     y = prop_safe_belief_percent, \n                     color = true_region)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.3) +\n  geom_jitter(width = 0.05, alpha = 0.1, show.legend = FALSE) +\n  labs(\n    title = \"Percent of People Who Believe Vaccines are Safe, \\nby Country and Global Region\",\n    subtitle = \"Vertical Lines Represent Region Medians\",\n    caption = \"Source: Wellcome Global Monitor, part of the Gallup World Poll 2018\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.major.x = element_line(color = \"lightgrey\",\n                                  linewidth = 0.3),\n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        legend.position = \"none\",\n        plot.caption = element_text(size = 7, hjust = 1, face = \"italic\")) +\n  coord_flip(ylim = c(20, 100))"
  },
  {
    "objectID": "stat-541-posts/2025-04-30-lab-2/index.html#part-two-broad-visualization-improvement",
    "href": "stat-541-posts/2025-04-30-lab-2/index.html#part-two-broad-visualization-improvement",
    "title": "Lab 2 - Advanced Data Visualizations",
    "section": "Part Two: Broad Visualization Improvement",
    "text": "Part Two: Broad Visualization Improvement\n\nChart 3.3: Map of countries according to levels of Trust in Scientists (pg. 55). Each country is supposed to display the amount of trust that a country has for scientists. It’s quite difficult to draw a lot of insights from this graph, but apparently we’re supposed to be able to recognize that countries with a lot of diversity tend to have a lot more trust in scientists.\nCountry, percentage of people who answered ‘high trust,’ percentage of ‘medium trust,’ and percentage of ‘low trust.’\nShape: country, color: trust percentage\nThis is a heatmap\n\n\n\nApparently, they displayed three different variables on the graph, the percentage of those that answered ‘high trust,’ ‘medium trust,’ and ‘low trust.’ However, I’m not quite sure how they managed to display this, as it’s on a sliding scale from low to high, rather than percentage. Based on their analysis, I have a feeling that they either just used the “high,” or just displayed the WGM_index. But, I will make this more clear.\nThe country colors all look very similar to each other. I believe I should make the colors for “low trust” and “high trust” more distinct.\nI think I should have an option to hover over the country and have it display its trust level. It’s difficult identifying which one has the largest trust.\n\n\n\n\n\nfinal_dataset_2 &lt;- cleaned_dataset |&gt; \n  group_by(Country) |&gt; \n  summarize(\n    total = n(),\n    prop_low_trust = (sum(WGM_Indexr == 1, na.rm = TRUE) / total) * 100,\n    prop_med_trust = (sum(WGM_Indexr == 2, na.rm = TRUE) / total) * 100,\n    prop_high_trust = (sum(WGM_Indexr == 3, na.rm = TRUE) / total) * 100,\n    prop_no_opinion = (sum(WGM_Indexr == 99, na.rm = TRUE) / total) * 100,\n    avg_trust = mean(WGM_Index, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n\nfinal_dataset_2 &lt;- final_dataset_2 |&gt; \n  mutate(Country = case_when(\n    Country == \"United States\" ~ \"United States of America\",\n    Country == \"Czech Republic\" ~ \"Czechia\",\n    Country == \"Ivory Coast\" ~ \"Côte d'Ivoire\",\n    Country == \"Republic of Congo\" ~ \"Dem. Rep. Congo\",\n    TRUE ~ Country\n  ))\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nmap_data &lt;- world |&gt;\n  left_join(final_dataset_2, by = c(\"name\" = \"Country\")) |&gt; \n  filter(name != \"Antarctica\") \n\n\npal &lt;- colorNumeric(\n  palette = \"magma\",\n  domain = c(1, 4),\n  reverse = TRUE,\n  na.color = \"grey\"\n)\n\nleaflet(data = map_data) |&gt; \n  addTiles() |&gt; \n  addPolygons(\n    fillColor = ~pal(avg_trust),\n    color = \"white\",\n    weight = 0.5,\n    fillOpacity = 0.8,\n    popup = ~paste0(\n      \"&lt;b&gt;\", name, \"&lt;/b&gt;&lt;br&gt;\",\n      \"High Trust: \", round(prop_high_trust, 2), \"%&lt;br&gt;\",\n      \"Medium Trust: \", round(prop_med_trust, 2), \"%&lt;br&gt;\",\n      \"Low Trust: \", round(prop_low_trust, 2), \"%&lt;br&gt;\",\n      \"No Opinion: \", round(prop_no_opinion, 2), \"%&lt;br&gt;\",\n      \"Avg Science Trust Index: \", round(avg_trust, 2)\n    ),\n    label = ~paste(name, \"Average Trust in Science Index: \", round(avg_trust, 2))\n  ) |&gt; \n  addLegend(pal = pal, values = c(1, 4), title = \"Average Trust in Science Index\")"
  },
  {
    "objectID": "stat-541-posts/2025-04-30-lab-2/index.html#part-three-third-data-visualization-improvement",
    "href": "stat-541-posts/2025-04-30-lab-2/index.html#part-three-third-data-visualization-improvement",
    "title": "Lab 2 - Advanced Data Visualizations",
    "section": "Part Three: Third Data Visualization Improvement",
    "text": "Part Three: Third Data Visualization Improvement\n\nChart 3.8: Scatterplot exploring the relationship between a country’s life expectancy at birth and people who trust doctors and nurses (pg. 101). This graph is trying to show that there is a positive relationship between the average life expectancy of a country, and how much they trust doctors and nurses. As one increases, the other does as well.\nCountry, average life expectancy, percentage of people who answered ‘a lot’ or ‘some.’\ny-axis: percentage of people who answered ‘a lot’ or ‘some,’ x-axis: average life expectancy, labels: country name\n\n\n\nThe axes are quite hard to read and are not centered\nWhile some countries are labeled for a reason, it appears as though most of the selected ones are arbitrary. I’ll try to add in a hover feature (which displays the country name and values) instead\n\n\n\n\n\nfinal_dataset_3 &lt;- cleaned_dataset |&gt;\n  group_by(Country) |&gt;\n  summarize(\n    n_trust_medic = sum(Q11E %in% c(1,2), na.rm = TRUE),\n    prop_trust_medic = sum(Q11E %in% c(1,2), na.rm = TRUE)/sum(!is.na(Q11E))\n  )\n\n\nlife &lt;- read_excel(\n  here::here(\"data\", \"Life.xls\"), \n  skip = 3,\n  sheet = \"Data\")\n\n\nlife &lt;- life |&gt; \n  select(`Country Name`, `2018`)\n\n\nfinal_dataset_3 &lt;- life |&gt;\n  left_join(final_dataset_3, by = c(\"Country Name\" = \"Country\"))\n\n\nfinal_dataset_3 &lt;- final_dataset_3 |&gt;  \n  filter(!is.na(prop_trust_medic))\n\nfit &lt;- lm(prop_trust_medic ~ `2018`, data = final_dataset_3) |&gt; fitted.values()\n\nplot &lt;- plot_ly(\n  data = final_dataset_3,\n  x = ~`2018`,\n  y = ~prop_trust_medic * 100,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~paste(\n    \"Country: \", `Country Name`, \"&lt;br&gt;\",\n    \"Proportion of Trust in Doctors and Nurses: \", round(prop_trust_medic * 100, 1), \"%&lt;br&gt;\",\n    \"Life Expectancy: \", `2018`\n  ),\n  hoverinfo = 'text',\n  marker = list(size = 10)\n)\n\nplot &lt;- layout(\n  plot,\n  title = list(\n    text = \"\\nRelationship Between a Country’s Life Expectancy and its Trust in Medical Professionals\\n \",\n    font = list(size = 15),\n    x = 0.07,\n    xanchor = \"left\"\n  ),\n  xaxis = list(\n    title = \"Life Expectancy at Birth (2018)\",\n    dtick = 10\n  ),\n  yaxis = list(\n    title = \"\"\n  )\n)\n\nplot"
  },
  {
    "objectID": "stat-541-posts/2025-05-27-lab-8/index.html",
    "href": "stat-541-posts/2025-05-27-lab-8/index.html",
    "title": "Lab 8 - Webscraping",
    "section": "",
    "text": "Note: This is an assignment that I reworked after I got some feedback on. It was initially a collaborative assignment, but the adjustments I made after receiving a grade were done independently. There are two GitHub links below, one which contains the initial assignment, and one that contains the finalized assignment. However, the final assignment’s code is listed here.\nView the collaborative code on GitHub\nView the final code on GitHub\n\nGoal: Scrape information from https://www.cheese.com to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. 🪤\n\n\nlibrary(purrr)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(rvest)\n\nPart 1: Locate and examine the robots.txt file for this website. Summarize what you learn from it.\nhttps://www.cheese.com/robots.txt User-agent: * Sitemap: https://www.cheese.com/sitemap.xml\nUser-agent: *: anyone is allowed to scrape No Crawl-delay: no wait time is required between each page scraped No Visit-time entry: no restrictions on time that scraping is allowed No Request-rate entry: no restrictions on simultaneous requests No mention of Disallow sections\nPart 2: Learn about the html_attr() function from rvest. Describe how this function works with a small example.\nIt gets the an attribute of a single html element. You can pull various details of an element, such as the reference link, the class, the name, the css styling etc. Below I have initialized a basic html. We stated that the reference link for the first element is a.com and that the class is ‘vital’. For the second we do the same but with href being alisa.com and the class being ‘541’. For the last we can see when the attribute is outside of the  it is not recognized as an attribute. Thus the class, “active” is not pulled, but the href, harshini.com is pulled.\n\n# below we initialize a basic html\nhtml &lt;- minimal_html('&lt;ul&gt;\n  &lt;li&gt;&lt;a href=\"https://cheese.com\" class=\"vital\"&gt;a&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=\"https://alisa.com\" class=\"541\" &gt;b&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;class=\"active\"&lt;a href=\"https://harshini.com\"&gt;b&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;')\n\n# here we run the function to pull the attributes we wrote\nhtml %&gt;% html_elements(\"a\") %&gt;% html_attrs()\n\n[[1]]\n                href                class \n\"https://cheese.com\"              \"vital\" \n\n[[2]]\n               href               class \n\"https://alisa.com\"               \"541\" \n\n[[3]]\n                  href \n\"https://harshini.com\" \n\n\nPart 3: (Do this alongside Part 4 below.) I used ChatGPT to start the process of scraping cheese information with the following prompt:\n\nWrite R code using the rvest package that allows me to scrape cheese information from cheese.com.\n\nFully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.\n\n# Load required libraries: already have a libraries code chunk, and reloading the libraries every time, feels inefficient, but not incredibly so. \nlibrary(rvest) \nlibrary(dplyr)\n\n\n# Define the URL: this seems to be correct and defining this individually makes it easier should we need to change the url or use it somewhere else\nurl &lt;- \"https://www.cheese.com/alphabetical\"\n\n# Read the HTML content from the webpage: \nwebpage &lt;- read_html(url)\n\n# Extract the cheese names and URLs\ncheese_data &lt;- webpage %&gt;%\n  html_nodes(\".cheese-item\") %&gt;%\n  # cheese-item is non-existent, this should be h3 a, as when we ran that we had legitimate output\n  html_nodes(\"a\") %&gt;%\n  html_attr(\"href\") %&gt;%\n  paste0(\"https://cheese.com\", .)\n  #the outcome of this is \"https://cheese.com\" which is not what we would want: so the paste0 function is useless\n  # the general structure is useful though, and when you separate the flow into two it seems to work\n\n\ncheese_names &lt;- webpage %&gt;%\n  html_nodes(\".cheese-item h3\") %&gt;%\n  # again .cheese-item is non-existent so we would want to remove that, but then it runs fine\n  html_text()\n\n\n# this works well, when the above is fixed\n# Create a data frame to store the results\ncheese_df &lt;- data.frame(Name = cheese_names,\n                        URL = cheese_data,\n                        stringsAsFactors = FALSE)\n\n\n# Print the data frame\nprint(cheese_df)\n\nPart 4: Obtain the following information for all cheeses in the database:\n\ncheese name\nURL for the cheese’s webpage (e.g., https://www.cheese.com/gouda/)\nwhether or not the cheese has a picture (e.g., gouda has a picture, but bianco does not).\n\nTo be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)\n\nget_text_from_page &lt;- function(page, css_selector) {\n    \n  page |&gt; \n    html_elements(css_selector) |&gt; \n    html_text()\n}\n\nscrape_page &lt;- function(url) {\n  \n  if (!is.character(url) || length(url) != 1 || !grepl(\"^https?://\", url)) { \n    #grepl reference: https://www.educative.io/answers/what-is-the-grepl-function-in-r\n    stop(\"inputted url must be a valid single URL string.\")\n  }\n    # 1 second pause between page queries\n    Sys.sleep(1)\n    \n    # Read the page \n    page &lt;- read_html(url)\n    \n    # Grab cheese name from the page\n    cheese_name &lt;- get_text_from_page(page, \"h3\")\n    \n    # Grab link from cheese node within page\n    cheese_href &lt;- page |&gt; \n      html_nodes(\"h3 a\") |&gt; \n      html_attr(\"href\")\n    \n    # make it look like a url\n    cheese_url &lt;- paste0(\"https://cheese.com\", cheese_href)\n    \n    # Grab the main body image elements\n    cheese_pic_reference &lt;- page |&gt; \n      html_elements(\"#main-body img\") \n    \n    # If the class of the cheese pic image is image-exists, then set cheese_pic_ifelse to true\n    cheese_pic_ifelse &lt;- cheese_pic_reference |&gt; \n      html_attr(\"class\") |&gt; \n      str_detect(\"image-exists\") \n    \n    #Make a tibble\n    tibble(\n        name = cheese_name,\n        url = cheese_url,\n        has_pic = cheese_pic_ifelse\n    )\n}\n\n\nbase_url &lt;- \"https://www.cheese.com/alphabetical/?per_page=100\"\n\nurls_all_pages &lt;- c(base_url, \n                    str_c(base_url, \n                          \"&page=\", \n                          1:21)\n                    )\n\npages &lt;- map(urls_all_pages, scrape_page)\n\ndf_cheeses &lt;- bind_rows(pages)\n\nhead(df_cheeses)\n\n# A tibble: 6 × 3\n  name                              url                                  has_pic\n  &lt;chr&gt;                             &lt;chr&gt;                                &lt;lgl&gt;  \n1 2 Year Aged Cumin Gouda           https://cheese.com/2-year-aged-cumi… TRUE   \n2 3-Cheese Italian Blend            https://cheese.com/3-cheese-italian… FALSE  \n3 30 Month Aged Parmigiano Reggiano https://cheese.com/30-month-aged-pa… TRUE   \n4 3yrs Aged Vintage Gouda           https://cheese.com/3yrs-aged-vintag… TRUE   \n5 Aarewasser                        https://cheese.com/aarewasser/       TRUE   \n6 Abbaye de Belloc                  https://cheese.com/abbaye-de-belloc/ TRUE   \n\n\nPart 5: When you go to a particular cheese’s page (like gouda), you’ll see more detailed information about the cheese. For just 10 of the cheeses in the database, obtain the following detailed information:\n\nmilk information\ncountry of origin\nfamily\ntype\nflavour\n\n(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)\n\n#Extract a certain amount of cheese links per page\nextract_links &lt;- function(main_url, n = 10) {\n  #Input check\n  if (!is.numeric(n) || n &lt;= 0 || n != as.integer(n)) {\n    stop(\"n must be a positive integer.\")\n  }\n  \n  # 1 second pause between page queries\n  Sys.sleep(1)\n  \n  #Read the page\n  page &lt;- read_html(main_url)\n  \n  #Get the first n urls for cheeses\n  cheese_href &lt;- page |&gt;\n    html_nodes(\"h3 a\") |&gt; \n    head(n) |&gt;\n    html_attr(\"href\")\n  \n  #get cheese url\n  paste0(\"https://cheese.com\", cheese_href)\n}\n\n#Helper function to extract the specific cheese information\nextract_field &lt;- function(field_name, cheese_text) {\n  #Input check\n  if (!is.character(field_name) || length(field_name) != 1) {\n    stop(\"field_name must be a single string.\")\n  }\n  \n  # Search for lines starting with field\n  field &lt;- cheese_text[str_detect(cheese_text, paste0(\"^\", field_name))]\n  \n  if (length(field) == 0) return(NA)\n  \n  trimws(str_remove(field, field_name))\n}\n\n#Scrape each individual cheese page\ncheese_scrape &lt;- function(cheese_url) {\n #1 second pause between page queries\n  Sys.sleep(1)\n  \n  #Read the page\n  page &lt;- read_html(cheese_url)\n    \n  # Get the cheese stuff\n  cheese_info_items &lt;- page |&gt; \n    html_elements(\"li p\") |&gt; \n    html_text()\n    \n  tibble(\n    milk = extract_field(\"Made from\", cheese_info_items),\n    country = extract_field(\"Country of origin:\", cheese_info_items),\n    family = extract_field(\"Family:\", cheese_info_items),\n    type = extract_field(\"Type:\", cheese_info_items),\n    flavour = extract_field(\"Flavour:\", cheese_info_items)\n  )\n}\n\n\ncheese_urls &lt;- extract_links(\"https://www.cheese.com/alphabetical/\")\n\npages2 &lt;- map(cheese_urls, cheese_scrape)\n\ndf_cheeses2 &lt;- bind_rows(pages2)\n\n\ndf_cheeses2\n\n# A tibble: 10 × 5\n   milk                       country     family   type                  flavour\n   &lt;chr&gt;                      &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;                 &lt;chr&gt;  \n 1 pasteurized cow's milk     Netherlands &lt;NA&gt;     semi-hard             sharp  \n 2 pasteurized cow's milk     Italy       Parmesan semi-soft, artisan    butter…\n 3 unpasteurized cow's milk   Italy       &lt;NA&gt;     hard                  &lt;NA&gt;   \n 4 pasteurized cow's milk     Netherlands &lt;NA&gt;     hard                  strong \n 5 unpasteurized cow's milk   Switzerland &lt;NA&gt;     semi-soft             sweet  \n 6 unpasteurized sheep's milk France      &lt;NA&gt;     semi-hard, artisan    burnt …\n 7 cow's milk                 France      &lt;NA&gt;     semi-hard             &lt;NA&gt;   \n 8 unpasteurized cow's milk   France      &lt;NA&gt;     semi-soft, artisan, … acidic…\n 9 unpasteurized cow's milk   France      &lt;NA&gt;     soft, artisan         fruity…\n10 pasteurized cow's milk     France      &lt;NA&gt;     semi-hard             salty,…\n\n\nPart 6: Evaluate the code that you wrote in terms of efficiency. To what extent do your function(s) adhere to the principles for writing good functions? To what extent are your functions efficient? To what extent is your iteration of these functions efficient?\nOur code for scraping cheese data from cheese.com is fairly efficient and follows several principles of writing good functions. First of all, our functions are well-named, each with a single clear purpose. Scrape_page() extracts cheese names, URLs, and image existence, while cheese_scrape() focuses on extracting detailed cheese characteristics from individual pages. This separation improved reusability, and allows our function to be cleaner. We did our best to make our functions flexible. For example, extract_links() allows us to set how many cheese URLs to extract. Efficiency-wise, the code uses vectorized operations to the best of our knowledge and avoids repeated work, such as reloading libraries or scraping unnecessary pages. We used map() for iteration over multiple pages ensuring cleaner application of functions across a list of URLs. Additionally, adding the Sys.sleep(1) pause respects the server’s resources. One area for potential improvement could be if we optimized the scraping logic to avoid redundant parsing, some operations could be done once per page instead od multiple times per element.\nOverall, I think we did pretty good, but there are still ways we could make our code more efficient!"
  },
  {
    "objectID": "stat-541-posts/2025-04-30-lab-1/index.html",
    "href": "stat-541-posts/2025-04-30-lab-1/index.html",
    "title": "Lab 1 - Reviewing Quarto, ggplot, and dplyr",
    "section": "",
    "text": "View the code on GitHub\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggridges)"
  },
  {
    "objectID": "stat-541-posts/2025-04-30-lab-1/index.html#quarto",
    "href": "stat-541-posts/2025-04-30-lab-1/index.html#quarto",
    "title": "Lab 1 - Reviewing Quarto, ggplot, and dplyr",
    "section": "Quarto",
    "text": "Quarto\nFirst, let’s make sure you know how to use Markdown formatting to style a Quarto document.\n\nMake this text bold.\nMake this text italicized.\nMake these into a bullet point list:\n\n\nApples\nBananas\nPotatoes\n\n\nEdit the YAML to remove warning messages from being output in the rendered HTML file\nUsing code chunk options, make it so this chunk shows the plot but not the source code:\n\n\n\n\n\n\n\n\n\n\n\nUsing code chunk options, remove the messages about bandwidth geom_density_ridges() chose to use:\n\n\nggplot(data = mpg, \n       mapping = aes(y = manufacturer, x = hwy)) + \n  geom_density_ridges() +\n  labs(x = \"\",\n       y = \"\", \n       title = \"Highway Milage (mpg) for Different Car Manufacturers\"\n       )\n\n\n\n\n\n\n\n\n\nUsing code chunk options, make it so that these plots are printed side-by-side:\n\nggplot(data = mpg, \n       mapping = aes(y = manufacturer, x = hwy)) + \n  geom_boxplot() +\n  labs(x = \"\",\n       y = \"\", \n       title = \"Highway Milage (mpg) for Different Car Manufacturers\"\n       )\nggplot(data = mpg, \n       mapping = aes(y = manufacturer, x = hwy)) + \n  geom_density_ridges() +\n  labs(x = \"\",\n       y = \"\", \n       title = \"Highway Milage (mpg) for Different Car Manufacturers\"\n       )\n\nPicking joint bandwidth of 1.31"
  },
  {
    "objectID": "stat-541-posts/2025-05-04-lab-3/index.html",
    "href": "stat-541-posts/2025-05-04-lab-3/index.html",
    "title": "Lab 3 - Static Quarto Dashboards",
    "section": "",
    "text": "Since this is a Quarto dashboard rather than an html file, I can’t publish it here. But, I included the link to where the dashboard is published on Quarto Pub.\nHere’s the link to the dashboard\nView the code on GitHub"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hello, and welcome to my website! My name is Alisa, and I am a Master’s student at Cal Poly. I am currently studying statistics, and minoring in philosophy and computer science. This website is where I show off some projects, some done for class, and some for my own development. I hope you find my work interesting!"
  },
  {
    "objectID": "personal-projects.html",
    "href": "personal-projects.html",
    "title": "Personal Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "stat-541-posts.html",
    "href": "stat-541-posts.html",
    "title": "STAT 541",
    "section": "",
    "text": "Lab 9 - Generative Art\n\n\n\nData Visualization\n\nFunctions\n\n\n\nThis lab was mostly for fun, and I mostly treated it as a creative writing assignment (I am a philosophy minor, after all!). But, this really did teach me a lot. I really…\n\n\n\n\n\n\nJun 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 8 - Webscraping\n\n\n\nFunctions\n\nWebscraping\n\nData Cleaning\n\n\n\nWithin this lab, we were taught how to webscrape, and the best practices on webscraping. We then used our new skills to gather information from the cheese.com website\n\n\n\n\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 7 - Using APIs\n\n\n\nFunctions\n\nData Visualization\n\nData Cleaning\n\nAPIs\n\n\n\nWithin this lab, we were taught how to use APIs and were asked to use them to visualize some ISS information. This involved some function writing, pulling APIs, and…\n\n\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 6 - Code Efficiency\n\n\n\nFunctions\n\n\n\nWithin this lab, we were taught some best-practices for to creating functions. Then, we were asked to write functions as efficiently as we can. This was one of my first…\n\n\n\n\n\n\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 4 - Dynamic Quarto Dashboards\n\n\n\nData Visualization\n\nData Cleaning\n\nShiny\n\n\n\nThis lab once again involved expanding upon the work done in the previous lab, where I created a static Quarto dashboard. Now that we had a baseline setup of our dashboard…\n\n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 3 - Static Quarto Dashboards\n\n\n\nData Visualization\n\nData Cleaning\n\nQuarto\n\n\n\nThis lab involved expanding upon the work done in the previous lab. We had to use the visualizations that we created and make a static dashboard that would show off our work.\n\n\n\n\n\n\nApr 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 2 - Advanced Data Visualizations\n\n\n\nData Visualization\n\nData Cleaning\n\n\n\nThis lab involved reviewing and improving upon visualizations from the Wellcome Global Monitor 2018 Report. This involved data cleaning, identifying grammar of graphics, and…\n\n\n\n\n\n\nApr 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 1 - Reviewing Quarto, ggplot, and dplyr\n\n\n\nData Visualization\n\nQuarto\n\n\n\nThis is the very first lab of STAT 541 and acted as a warm up to ensure we were familiar with some more basic Quarto functionality.\n\n\n\n\n\n\nApr 7, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "stat-541-posts/2025-05-12-lab-6/index.html",
    "href": "stat-541-posts/2025-05-12-lab-6/index.html",
    "title": "Lab 6 - Code Efficiency",
    "section": "",
    "text": "Note: This lab was submitted and then adjusted after I received some feedback on it. The GitHub link should contain all of the versions, but the code displayed here is just my final version.\nView the code on GitHub"
  },
  {
    "objectID": "stat-541-posts/2025-05-12-lab-6/index.html#testing-your-function",
    "href": "stat-541-posts/2025-05-12-lab-6/index.html#testing-your-function",
    "title": "Lab 6 - Code Efficiency",
    "section": "Testing Your Function!",
    "text": "Testing Your Function!\n\n## Testing how your function handles multiple input variables\nremove_outliers(diamonds, \n                price, \n                x, \n                y, \n                z)\n\n       carat       cut color clarity depth table price     x     y     z\n       &lt;num&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;ord&gt; &lt;num&gt; &lt;num&gt; &lt;int&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n    1:  0.23     Ideal     E     SI2  61.5    55   326  3.95  3.98  2.43\n    2:  0.21   Premium     E     SI1  59.8    61   326  3.89  3.84  2.31\n    3:  0.23      Good     E     VS1  56.9    65   327  4.05  4.07  2.31\n    4:  0.29   Premium     I     VS2  62.4    58   334  4.20  4.23  2.63\n    5:  0.31      Good     J     SI2  63.3    58   335  4.34  4.35  2.75\n   ---                                                                  \n52685:  0.72     Ideal     D     SI1  60.8    57  2757  5.75  5.76  3.50\n52686:  0.72      Good     D     SI1  63.1    55  2757  5.69  5.75  3.61\n52687:  0.70 Very Good     D     SI1  62.8    60  2757  5.66  5.68  3.56\n52688:  0.86   Premium     H     SI2  61.0    58  2757  6.15  6.12  3.74\n52689:  0.75     Ideal     D     SI2  62.2    55  2757  5.83  5.87  3.64\n\n## Testing how your function handles an input that isn't numeric\nremove_outliers(diamonds, \n                price, \n                color)\n\nWarning in remove_outliers(diamonds, price, color): The following columns are\nnot numeric and will be skipped: color\n\n\n       carat       cut color clarity depth table price     x     y     z\n       &lt;num&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;ord&gt; &lt;num&gt; &lt;num&gt; &lt;int&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n    1:  0.23     Ideal     E     SI2  61.5    55   326  3.95  3.98  2.43\n    2:  0.21   Premium     E     SI1  59.8    61   326  3.89  3.84  2.31\n    3:  0.23      Good     E     VS1  56.9    65   327  4.05  4.07  2.31\n    4:  0.29   Premium     I     VS2  62.4    58   334  4.20  4.23  2.63\n    5:  0.31      Good     J     SI2  63.3    58   335  4.34  4.35  2.75\n   ---                                                                  \n52730:  0.72     Ideal     D     SI1  60.8    57  2757  5.75  5.76  3.50\n52731:  0.72      Good     D     SI1  63.1    55  2757  5.69  5.75  3.61\n52732:  0.70 Very Good     D     SI1  62.8    60  2757  5.66  5.68  3.56\n52733:  0.86   Premium     H     SI2  61.0    58  2757  6.15  6.12  3.74\n52734:  0.75     Ideal     D     SI2  62.2    55  2757  5.83  5.87  3.64\n\n## Testing how your function handles a non-default sd_thresh\nremove_outliers(diamonds, \n                price,\n                x, \n                y, \n                z, \n                sd_thresh = 2)\n\n       carat       cut color clarity depth table price     x     y     z\n       &lt;num&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;ord&gt; &lt;num&gt; &lt;num&gt; &lt;int&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n    1:  0.23     Ideal     E     SI2  61.5    55   326  3.95  3.98  2.43\n    2:  0.21   Premium     E     SI1  59.8    61   326  3.89  3.84  2.31\n    3:  0.23      Good     E     VS1  56.9    65   327  4.05  4.07  2.31\n    4:  0.29   Premium     I     VS2  62.4    58   334  4.20  4.23  2.63\n    5:  0.31      Good     J     SI2  63.3    58   335  4.34  4.35  2.75\n   ---                                                                  \n50095:  0.72     Ideal     D     SI1  60.8    57  2757  5.75  5.76  3.50\n50096:  0.72      Good     D     SI1  63.1    55  2757  5.69  5.75  3.61\n50097:  0.70 Very Good     D     SI1  62.8    60  2757  5.66  5.68  3.56\n50098:  0.86   Premium     H     SI2  61.0    58  2757  6.15  6.12  3.74\n50099:  0.75     Ideal     D     SI2  62.2    55  2757  5.83  5.87  3.64\n\n\nExercise 2: Write a function that imputes missing values for numeric variables in a dataset. The user should be able to supply the dataset, the variables to impute values for, and a function to use when imputing. Hint 1: You will need to use across() to apply your function, since the user can input multiple variables. Hint 2: The replace_na() function is helpful here!\n\nimpute_missing_dt &lt;- function(data, ..., impute_fun = mean) { \n  #Capture column names\n  cols &lt;- as.character(substitute(list(...)))[-1]\n  \n  #Check for columns which are non-numeric\n  non_numeric &lt;- cols[!sapply(data[, cols, drop = FALSE], is.numeric)]\n  \n  if (length(non_numeric) &gt; 0) {\n    warning(\"The following columns are not numeric and will be skipped: \",\n            paste(non_numeric, collapse = \", \"))\n  }\n  \n  #Filter only numeric columns\n  numeric_cols &lt;- setdiff(cols, non_numeric)\n  \n  data |&gt; \n    mutate(across(\n      .cols = all_of(numeric_cols), \n      .fns = ~ replace_na(.x, impute_fun(.x, na.rm = TRUE)) #Impute using the specified function\n    ))\n}"
  },
  {
    "objectID": "stat-541-posts/2025-05-12-lab-6/index.html#testing-your-function-1",
    "href": "stat-541-posts/2025-05-12-lab-6/index.html#testing-your-function-1",
    "title": "Lab 6 - Code Efficiency",
    "section": "Testing Your Function!",
    "text": "Testing Your Function!\n\n## Testing how your function handles multiple input variables\nimpute_missing_dt(nycflights13::flights, \n               arr_delay, \n               dep_delay) \n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n## Testing how your function handles an input that isn't numeric\nimpute_missing_dt(nycflights13::flights, \n               arr_delay, \n               carrier)\n\nWarning in impute_missing_dt(nycflights13::flights, arr_delay, carrier): The\nfollowing columns are not numeric and will be skipped: carrier\n\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n## Testing how your function handles a non-default impute_fun\nimpute_missing_dt(nycflights13::flights, \n               arr_delay, \n               dep_delay, \n               impute_fun = median)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "stat-541-posts/2025-05-12-lab-6/index.html#testing-your-function-2",
    "href": "stat-541-posts/2025-05-12-lab-6/index.html#testing-your-function-2",
    "title": "Lab 6 - Code Efficiency",
    "section": "Testing Your Function!",
    "text": "Testing Your Function!\n\nfit_model(\n  diamonds,\n  mod_formula = price ~ carat + cut,\n  remove_outliers = TRUE,\n  impute_missing = TRUE,\n  price, \n  carat\n)\n\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2460.16      7526.96      1059.65      -410.54       295.80        82.62"
  },
  {
    "objectID": "stat-541-posts/2025-05-12-lab-6/index.html#parameters",
    "href": "stat-541-posts/2025-05-12-lab-6/index.html#parameters",
    "title": "Lab 6 - Code Efficiency",
    "section": "Parameters",
    "text": "Parameters\nFirst, we need to define the set of parameters we want to iterate the fit_model() function over. The tidyr package has a useful function called crossing() that is useful for generating argument combinations. For each argument, we specify all possible values for that argument and crossing() generates all combinations. Note that you can create a list of formula objects in R with c(y ~ x1, y ~ x1 + x2).\n\ndf_arg_combos &lt;- crossing(\n    impute = c(TRUE, FALSE),\n    remove_outliers = c(TRUE, FALSE), \n    mod = c(y ~ x1, \n            y ~ x1 + x2)\n)\ndf_arg_combos\n\nExercise 4: Use crossing() to create the data frame of argument combinations for our analyses.\n\ndf_arg_combos &lt;- crossing(\n  impute_missing = c(TRUE, FALSE),\n  remove_outliers = c(TRUE, FALSE),\n  mod_formula = list(\n    price ~ carat, #No additional variables\n    price ~ carat + cut, #Adjusting for cut\n    price ~ carat + cut + clarity, #Adjusting for cut and clarity\n    price ~ carat + cut + clarity + color #Adjusting for all variables\n  )\n)"
  },
  {
    "objectID": "stat-541-posts/2025-05-12-lab-6/index.html#iterating-over-the-parameters",
    "href": "stat-541-posts/2025-05-12-lab-6/index.html#iterating-over-the-parameters",
    "title": "Lab 6 - Code Efficiency",
    "section": "Iterating Over the Parameters",
    "text": "Iterating Over the Parameters\nWe’ve arrived at the final step!\nExercise 5: Use pmap() from purrr to apply the fit_model() function to every combination of arguments from `diamonds.\n\n#Apply fit_model to all combinations of arguments using pmap\nresults &lt;- pmap(df_arg_combos, fit_model, df = diamonds, price, carat)\n\n#Fitted models for each combination of arguments\nresults\n\n[[1]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat  \n      -2256         7756  \n\n\n[[2]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2701.38      7871.08      1239.80      -528.60       367.91        74.59  \n\n\n[[3]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3187.540     8472.026      713.804     -334.503      188.482        1.663  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4011.681    -1821.922      917.658     -430.047      257.141       26.909  \n  clarity^7  \n    186.742  \n\n\n[[4]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3710.603     8886.129      698.907     -327.686      180.565       -1.207  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4217.535    -1832.406      923.273     -361.995      216.616        2.105  \n  clarity^7      color.L      color.Q      color.C      color^4      color^5  \n    110.340    -1910.288     -627.954     -171.960       21.678      -85.943  \n    color^6  \n    -49.986  \n\n\n[[5]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat  \n      -2067         7411  \n\n\n[[6]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2460.16      7526.96      1059.65      -410.54       295.80        82.62  \n\n\n[[7]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2897.60      8118.13       618.65      -268.13       148.57         8.10  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n    3463.17     -1505.52       609.45      -286.81       158.48        71.83  \n  clarity^7  \n     183.33  \n\n\n[[8]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3376.856     8513.377      607.668     -263.835      142.453        5.506  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   3654.785    -1503.171      612.049     -225.568      124.920       47.356  \n  clarity^7      color.L      color.Q      color.C      color^4      color^5  \n    112.889    -1714.316     -539.239     -127.282       43.649      -60.866  \n    color^6  \n    -49.333  \n\n\n[[9]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat  \n      -2256         7756  \n\n\n[[10]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2701.38      7871.08      1239.80      -528.60       367.91        74.59  \n\n\n[[11]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3187.540     8472.026      713.804     -334.503      188.482        1.663  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4011.681    -1821.922      917.658     -430.047      257.141       26.909  \n  clarity^7  \n    186.742  \n\n\n[[12]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3710.603     8886.129      698.907     -327.686      180.565       -1.207  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4217.535    -1832.406      923.273     -361.995      216.616        2.105  \n  clarity^7      color.L      color.Q      color.C      color^4      color^5  \n    110.340    -1910.288     -627.954     -171.960       21.678      -85.943  \n    color^6  \n    -49.986  \n\n\n[[13]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat  \n      -2067         7411  \n\n\n[[14]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2460.16      7526.96      1059.65      -410.54       295.80        82.62  \n\n\n[[15]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2897.60      8118.13       618.65      -268.13       148.57         8.10  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n    3463.17     -1505.52       609.45      -286.81       158.48        71.83  \n  clarity^7  \n     183.33  \n\n\n[[16]]\n\nCall:\nlm(formula = mod_formula, data = df)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3376.856     8513.377      607.668     -263.835      142.453        5.506  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   3654.785    -1503.171      612.049     -225.568      124.920       47.356  \n  clarity^7      color.L      color.Q      color.C      color^4      color^5  \n    112.889    -1714.316     -539.239     -127.282       43.649      -60.866  \n    color^6  \n    -49.333"
  },
  {
    "objectID": "stat-541-posts/2025-05-04-lab-4/index.html",
    "href": "stat-541-posts/2025-05-04-lab-4/index.html",
    "title": "Lab 4 - Dynamic Quarto Dashboards",
    "section": "",
    "text": "Since this is a Shiny app rather than an html file, I can’t publish it here. But, I included the link to where the dashboard is published on shinyapps.io.\nHere’s the link to the Shiny app\nView the code on GitHub"
  },
  {
    "objectID": "stat-541-posts/2025-06-02-lab-9/index.html",
    "href": "stat-541-posts/2025-06-02-lab-9/index.html",
    "title": "Lab 9 - Generative Art",
    "section": "",
    "text": "View the code on GitHub\n\nThe Iris\n\niris |&gt; \n  ggplot() + \n  geom_violin(aes(Petal.Width, Petal.Length, colour = Species), size = 25, show.legend = FALSE) +\n  geom_violin(aes(Sepal.Width, Sepal.Length, colour = Species), size = 25, show.legend = FALSE) +\n  geom_smooth(aes(Petal.Width, Petal.Length, colour = Species), method = \"loess\", size = 2, se = FALSE, show.legend = FALSE, color = \"orange\") + \n  geom_jitter(aes(Petal.Width, Petal.Length, colour = Species), size = 2, color = \"#FDB915\") +\n  coord_polar() +\n  theme_void() + \n  scale_color_manual(values = c(\n    setosa = \"#5A4FCF\",\n    versicolor = \"#ce70e8\",\n    virginica = \"#a347e5\"\n  )) \n\n\n\n\n\n\n\n\nThe Iris is a tribute and transcendence. A data-born organism that blossoms with statistical soul. Forged from the humble R iris dataset, an archive of petal lengths and sepal widths, this piece spirals into abstraction, where color and curve take the place of taxonomy. In rich violets and ethereal lavenders, the form evokes the layered petals of its namesake flower. Yet from its digital stamen bursts a constellation of golden nodes. Not pollen, but precision. Each dot a data point, each arc a trace of algorithmic elegance. The composition suggests motion, intention, and growth, as if the very act of classification were blooming into consciousness. Neither purely natural nor wholly synthetic, The Iris exists at the intersection of art and analysis. It does not ask to be interpreted. It demands to be experienced. For in its radiant whorl lies the revelation that even from the smallest measurement, something luminous can unfold.\nThe Iris was created using the iris dataset in R, but instead of focusing on traditional analysis, the code transforms statistical summaries into abstract, floral forms. Large, layered violins represent the distributions of sepal and petal measurements, drawn with a size of 25 to emphasize their abstract, petal-like shapes over actual precision and data visualizations. Mapping colors to species adds visual contrast and variety, using soft purples and deep violets to make the the plot more remniscent of an iris. A bright orange loess line curves through the center, offering a sense of direction or movement through the swirling patterns. Yellow jittered points scatter like pollen across the petals, showing individual data points and adding a sense of randomness and life. The use of coord_polar() transforms the rectangular plot into a circular composition, making it resemble an iris in bloom. With axes removed and colors carefully chosen, the image becomes less about numbers and more about organic symmetry, turning data into something unexpectedly delicate and expressive.\n\n\nGeode of a Dying Star\n\n#Setting colors\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\n\n#Function that produces coordinates that get put into other noise processes\ntransform_to_curl_space &lt;- function(x, y, frequency = 1, octaves = 10) {\n  curl_noise(\n    generator = fracture,\n    noise = gen_simplex,\n    fractal = fbm,\n    octaves = octaves,\n    frequency = frequency,\n    x = x,\n    y = y\n  )\n}\n\n\n#Create cells\ndefine_worley_cells &lt;- function(x, y, frequency = 3, octaves = 6) {\n  fracture(\n    noise = gen_worley,\n    fractal = billow,\n    octaves = octaves,\n    frequency = frequency,\n    value = \"cell\",\n    x = x,\n    y = y\n  ) |&gt;\n    rank() |&gt; \n    normalise()\n}\n\n\n#Create noise\nsimplex_noise &lt;- function(x, y, frequency = .1, octaves = 10) {\n  fracture(\n    noise = gen_simplex,\n    fractal = ridged,\n    octaves = octaves,\n    frequency = frequency,\n    x = x,\n    y = y\n  ) |&gt;\n    normalise()\n}\n\n\nice_floe &lt;- function(seed) {\n  \n  set.seed(seed)\n  \n  grid &lt;- long_grid(\n    x = seq(0, 1, length.out = 2000), \n    y = seq(0, 1, length.out = 2000)\n  )\n  \n  coords &lt;- transform_to_curl_space(grid$x, grid$y)\n  \n  grid |&gt;\n    mutate(\n      cells = define_worley_cells(coords$x, coords$y, frequency = 8, octaves = 17), #Set frequency to 8, octaves to 17\n      paint = simplex_noise(x + cells, y + cells, frequency = 0.2), #Set frequency to 0.2\n      paint = normalise(paint)\n    ) |&gt;\n    as.array(value = paint)\n}\n\n\nshaded_ice_floe &lt;- function(seed) {\n  \n  art &lt;- ice_floe(seed)\n  \n  height_shade(\n    heightmap = art,\n    texture = sample_canva2(seed = 600, n = 800) #Made it pink\n  ) |&gt;\n    add_shadow(\n      shadowmap = ray_shade(\n        heightmap = art, \n        sunaltitude = 40, #Make the sun a little further\n        sunangle = 70,\n        multicore = TRUE, \n        zscale = .005\n      ), \n      max_darken = .3 #I want it pretty light\n    ) |&gt;\n    plot_map()\n}\n\nshaded_ice_floe(11032003)\n\n\n\n\n\n\n\n\nIn Geode of a Dying Star, the viewer is thrust into the violent stillness of cosmic decay. Where time folds in on itself and color is memory made visible. The concentric ruptures, seething in molten crimsons and collapsing oranges, evoke the final breath of a celestial body imploding under the weight of its own grandeur. This is not a landscape. It is a wound in spacetime. Each striation whispers of pressure long endured, of centuries pressed into silence. The work refuses serenity. It demands awe. It is a relic of a universe unraveling and reassembling itself through sheer, incandescent will. Scholars have debated whether this piece is a warning, a lament, or a hymn. Most agree on only one thing: it humbles the gaze.\nGeode of a Dying Star is a generative piece built from layers of mathematical noise and texture. The number 11032003, my birthday, is used as the master seed that governs the random generation of the entire piece. The structure begins with a 2000x2000 grid of points. Each point is pushed into a swirling pattern using a function called transform_to_curl_space(), which applies fractal noise to bend space into organic currents. Over this, Worley noise is used to form cell-like, cracked textures. It’s layered at a high frequency (8) and with many octaves (17). This creates sharp boundaries that mimic shattered surfaces. Color is painted on using simplex noise, shifted slightly by the Worley texture itself, giving the piece a sense of dynamic layering. The frequency is set to 0.2 to avoid overly fine grain, maintaining a soft, molten quality. To color the surface, the code samples from a randomly selected Canva palette using a fixed seed (600), then stretches that palette into 800 distinct tones. The seed was deliberately chosen to fill the picture with hues of soft pink, coral, and orange. Once shaped and colored, the picture is transformed into a heightmap. Shadows are added using simulated sunlight at a low angle (sun altitude of 40) and gentle intensity (max darken 0.3), casting soft ridges across the surface."
  },
  {
    "objectID": "stat-541-posts/2025-05-19-lab-7/index.html",
    "href": "stat-541-posts/2025-05-19-lab-7/index.html",
    "title": "Lab 7 - Using APIs",
    "section": "",
    "text": "Note: This is an assignment that I reworked after I got some feedback on. It was initially a collaborative assignment, but the adjustments I made after receiving a grade were done independently. There are two GitHub links below, one which contains the initial assignment, and one that contains the finalized assignment. However, the final assignment’s code is listed here.\nView the collaborative code on GitHub\nView the final code on GitHub"
  },
  {
    "objectID": "stat-541-posts/2025-05-19-lab-7/index.html#making-the-u.s.capitals-dataset",
    "href": "stat-541-posts/2025-05-19-lab-7/index.html#making-the-u.s.capitals-dataset",
    "title": "Lab 7 - Using APIs",
    "section": "Making the U.S.Capitals dataset",
    "text": "Making the U.S.Capitals dataset\n\ncapitals_names &lt;- read_lines(\"https://people.sc.fsu.edu/~jburkardt/datasets/states/state_capitals_name.txt\")\ncapitals_lat_long &lt;- read_lines(\"https://people.sc.fsu.edu/~jburkardt/datasets/states/state_capitals_ll.txt\")\n\nWe first created a dataset that contains all of the capital names, as well as their latitude and longitude coordinates.\n\nlatlon_df &lt;- str_split(capitals_lat_long, \"\\\\s+\", simplify = TRUE) |&gt;  \n  as.data.frame() |&gt; \n  rename(state = V1, latitude = V2, longitude = V3) |&gt; \n  mutate(state = trimws(state))\n\ncapitals_df &lt;- str_split(capitals_names, '\"', simplify = TRUE) |&gt;  \n  as.data.frame() |&gt; \n  rename(state = V1, capital = V2) |&gt; \n  select(-V3) |&gt; \n  mutate(state = trimws(state))\n\nfull_capitals_dataset &lt;- left_join(latlon_df, capitals_df, by = \"state\")\n#Technically a \"join\" is not necessary, since the columns are ordered the same, but we were worried it was going to cause issues\n\nfull_capitals_dataset &lt;- full_capitals_dataset |&gt; \n  mutate(\n    latitude = as.numeric(latitude),\n    longitude = as.numeric(longitude)\n  )"
  },
  {
    "objectID": "stat-541-posts/2025-05-19-lab-7/index.html#calling-an-api-to-get-pass-times",
    "href": "stat-541-posts/2025-05-19-lab-7/index.html#calling-an-api-to-get-pass-times",
    "title": "Lab 7 - Using APIs",
    "section": "Calling an API to get pass times",
    "text": "Calling an API to get pass times\nWe then created a function that inputs latitudes and longitudes, and retrieves the next three times that the ISS passes over that location. Note that the API has a maximum of 72 hours. So, if the ISS does not pass over a location within 72 hours, it will return NA. Additionally, because this API is time dependent, re-running it after some time may produce new results.\nNote: We used tca or “time of closest approach” as our pass time, but the API can also provide “start time” and “end time.” These are only about 10 minutes apart, however, so whichever value is chosen should not make a big difference.\n\nget_pass_times &lt;- function(lat, lon) {\n  #Check inputs: lat and lon should be numeric (I had to Google what ranges they could have, did not know they were on two different scales!)\n  if (!is.numeric(lat) || lat &lt; -90 || lat &gt; 90) {\n    warning(\"Invalid latitude. Must be a numeric value between -90 and 90.\")\n    return(rep(NA, 3))\n  }\n  if (!is.numeric(lon) || lon &lt; -180 || lon &gt; 180) {\n    warning(\"Invalid longitude. Must be a numeric value between -180 and 180.\")\n    return(rep(NA, 3))\n  }\n  \n  #Construct the API URL for the given latitude and longitude\n  url &lt;- paste0(\"https://api.g7vrd.co.uk/v1/satellite-passes/25544/\", lat, \"/\", lon, \".json\")\n  response &lt;- GET(url) #Call the API \n  \n\n  if (status_code(response) == 200) {\n    data &lt;- fromJSON(rawToChar(response$content)) #Convert raw JSON to usable form\n  \n  if (!is.null(data$passes)) {\n    return(head(data$passes$tca, 3)) #Extract first three pass times (Ordered descending)\n    } else {\n      warning(\"No passes found in API response.\")\n      }\n    } else {\n    warning(\"API request failed or returned non-200 status.\")\n    }\n\n  return(rep(NA, 3))  #Return 3 NAs if calling the API fails\n}\n\nWe now take our function and apply it to every row, thus producing a dataframe which contains the top three pass times for each capital. Because we wanted to limit our dataset to U.S. state capitals, we removed Puerto Rico. Additionally, there was an issue with the initial dataframe that we read in. It included two District of Columbias, one with the correct coordinates, and one with incorrect coordinates. We made sure to remove the incorrect one from the datset.\n\ncapitals_all_passes &lt;- full_capitals_dataset |&gt; \n  mutate(pass_times = pmap(list(latitude, longitude), get_pass_times)) \n  #pass_times is a list of values\n\ncapitals_with_passes &lt;- capitals_all_passes |&gt; \n  unnest_wider(pass_times, names_sep = \"_\") |&gt;  #Creates pass_times_1, pass_times_2, pass_times_3\n  rename(\n    pass_time_1 = pass_times_1,\n    pass_time_2 = pass_times_2,\n    pass_time_3 = pass_times_3\n  ) |&gt; \n  filter(!state %in% c('US', 'PR'))"
  },
  {
    "objectID": "stat-541-posts/2025-05-19-lab-7/index.html#making-the-iss-passing-visual",
    "href": "stat-541-posts/2025-05-19-lab-7/index.html#making-the-iss-passing-visual",
    "title": "Lab 7 - Using APIs",
    "section": "Making the ISS passing visual",
    "text": "Making the ISS passing visual\nThe following takes our dataset and creates a visualization.\n\ncapitals_with_passes &lt;- capitals_with_passes |&gt; \n  arrange(is.na(pass_time_1), pass_time_1) |&gt; \n  rowwise() |&gt; \n  mutate(\n    pop_up_html = if (is.na(pass_time_1)) {\n      paste0(\"&lt;b&gt;\", capital, \"&lt;/b&gt;&lt;br/&gt;\",\n             \"ISS will not pass over this location in the next 72 hours\")\n    } else {\n      paste0(\"&lt;b&gt;\", toupper(capital), \"&lt;/b&gt;&lt;br/&gt;\",\n             \"Next Three ISS Passtimes:&lt;br/&gt;\",\n             # read way many articles much about date time formatting and functions, but so happy with how it looks now\n             # https://learn.microsoft.com/en-us/dotnet/standard/base-types/standard-date-and-time-format-strings\n             format(as_datetime(pass_time_1), \"%B %d, %Y  %I:%M %p %Z\"),\n             \"&lt;br/&gt;\",\n             if_else(!is.na(pass_time_2),\n                     (format(as_datetime(pass_time_2), \"%B %d, %Y  %I:%M %p %Z\")),\n                     \"No second pass\"),\n             \"&lt;br/&gt;\",\n             if_else(!is.na(pass_time_3), (format(as_datetime(pass_time_3), \"%B %d, %Y  %I:%M %p %Z\")), \n                     \"No third pass\"))\n    },  \n    \n    label_html = htmltools::HTML((paste0(\n      \"&lt;b&gt;\", toupper(capital), \"&lt;b&gt;&lt;br/&gt;\",\n      if_else(is.na(pass_time_1),\n              \"ISS will not pass over this location in the next 72 hours\",\n              paste(\"Next ISS Passtime:\",\n                    format(as_datetime(pass_time_1), \"%B %d, %Y  %I:%M %p %Z\"))\n     ))))\n  ) |&gt; \n  ungroup()\n\n# created new columns in the dataframe, because the html options were having issues running within the leaflet plot and to make the code for the plot cleaner\n\n\n#dropping NAs from the path\ncaps&lt;- capitals_with_passes|&gt; filter(!is.na(pass_time_1))\n\n\niss_icon &lt;- icons(\n  iconUrl = here::here(\"stat-541-posts/2025-05-19-lab-7/pngtree-space-station-probe-icon-png-image_4687961.png\"), \n  iconWidth = 30, \n  iconHeight = 30\n)\n\nleaflet(capitals_with_passes) |&gt; addTiles() |&gt; \n  addMarkers(lng = ~longitude,\n             lat = ~latitude,\n             icon = iss_icon,\n             label = ~label_html,\n             popup = ~pop_up_html\n            # so excited to have discovered the use of the tilda('~')\n             ) |&gt;\n  addPolylines(lng = caps$longitude,\n               lat = caps$latitude, \n               color = \"pink\", #I made the lines pink because it's coquette\n               opacity = 1)\n\n\n\n\n\nLooks so good!"
  }
]